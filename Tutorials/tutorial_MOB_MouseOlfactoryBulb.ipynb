{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COSMOS Tutorial: Mouse Olfactory Bulb (MOB) Dataset\n",
    "\n",
    "## Dataset Overview\n",
    "- **Tissue**: Mouse Olfactory Bulb (MOB)\n",
    "- **Technology**: Spatial transcriptomics\n",
    "- **Cells**: ~3000-5000 cells (estimated)\n",
    "- **Layers**: ONL (Outer Nuclear Layer) + others\n",
    "- **Goal**: Compare GCN vs Graph Transformer + PE performance\n",
    "\n",
    "## Modality Split Strategy\n",
    "Since we only have gene expression data, we'll split genes into two artificial modalities:\n",
    "- **Modality 1**: High variance genes (top 50%)\n",
    "- **Modality 2**: Remaining genes (bottom 50%)\n",
    "\n",
    "This simulates multi-modal integration and is a common practice for testing spatial integration methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from umap import UMAP\n",
    "import sklearn\n",
    "import sklearn.metrics\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random_seed = 42\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Explore MOB Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "counts = pd.read_csv('./MOB_counts.csv', index_col=0)\n",
    "metadata = pd.read_csv('./MOB_meta.csv', index_col=0)\n",
    "\n",
    "print(f\"Counts shape: {counts.shape}\")\n",
    "print(f\"Metadata shape: {metadata.shape}\")\n",
    "print(f\"\\nMetadata columns: {metadata.columns.tolist()}\")\n",
    "print(f\"\\nLayer types: {metadata['Layertype'].unique()}\")\n",
    "print(f\"Number of cells per layer:\")\n",
    "print(metadata['Layertype'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract spatial coordinates\n",
    "spatial_coords = metadata[['x_pos', 'y_pos']].values\n",
    "\n",
    "# Get layer annotations\n",
    "layer_annotations = metadata['Layertype'].values\n",
    "\n",
    "print(f\"Spatial coordinates shape: {spatial_coords.shape}\")\n",
    "print(f\"Coordinate range:\")\n",
    "print(f\"  X: [{spatial_coords[:, 0].min():.2f}, {spatial_coords[:, 0].max():.2f}]\")\n",
    "print(f\"  Y: [{spatial_coords[:, 1].min():.2f}, {spatial_coords[:, 1].max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Visualize Ground Truth Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ground truth spatial distribution\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 5))\n",
    "\n",
    "# Get unique layers and assign colors\n",
    "unique_layers = np.unique(layer_annotations)\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(unique_layers)))\n",
    "layer_to_color = dict(zip(unique_layers, colors))\n",
    "\n",
    "for layer in unique_layers:\n",
    "    mask = layer_annotations == layer\n",
    "    ax.scatter(spatial_coords[mask, 0], spatial_coords[mask, 1], \n",
    "               c=[layer_to_color[layer]], label=layer, s=10, alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('X position')\n",
    "ax.set_ylabel('Y position')\n",
    "ax.set_title('MOB: Manual Annotation (Ground Truth)', fontsize=12, fontweight='bold')\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', frameon=False)\n",
    "ax.axis('equal')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Ground truth visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocess and Split Data into Two Modalities\n",
    "\n",
    "We'll split genes based on variance:\n",
    "- **Modality 1**: High-variance genes (informative)\n",
    "- **Modality 2**: Lower-variance genes (complementary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create AnnData object for initial processing\n",
    "adata_full = sc.AnnData(counts.values)\n",
    "adata_full.obs['Layertype'] = layer_annotations\n",
    "adata_full.obsm['spatial'] = spatial_coords\n",
    "adata_full.var_names = counts.columns\n",
    "adata_full.obs_names = counts.index\n",
    "\n",
    "print(f\"Full dataset: {adata_full.shape}\")\n",
    "\n",
    "# Basic preprocessing\n",
    "sc.pp.normalize_total(adata_full, target_sum=1e4)\n",
    "sc.pp.log1p(adata_full)\n",
    "\n",
    "# Calculate gene variance\n",
    "gene_var = np.var(adata_full.X, axis=0)\n",
    "if hasattr(gene_var, 'A1'):  # Handle sparse matrices\n",
    "    gene_var = gene_var.A1\n",
    "\n",
    "# Split genes by variance\n",
    "n_genes = len(gene_var)\n",
    "var_threshold = np.median(gene_var)\n",
    "\n",
    "high_var_genes = gene_var >= var_threshold\n",
    "low_var_genes = ~high_var_genes\n",
    "\n",
    "print(f\"\\nGene split:\")\n",
    "print(f\"  Modality 1 (high variance): {high_var_genes.sum()} genes\")\n",
    "print(f\"  Modality 2 (low variance): {low_var_genes.sum()} genes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two modality AnnData objects\n",
    "# Modality 1: High variance genes\n",
    "if hasattr(adata_full.X, 'toarray'):\n",
    "    data1 = adata_full.X.toarray()[:, high_var_genes]\n",
    "    data2 = adata_full.X.toarray()[:, low_var_genes]\n",
    "else:\n",
    "    data1 = adata_full.X[:, high_var_genes]\n",
    "    data2 = adata_full.X[:, low_var_genes]\n",
    "\n",
    "adata1 = sc.AnnData(data1, dtype='float64')\n",
    "adata1.obs['Layertype'] = layer_annotations\n",
    "adata1.obsm['spatial'] = spatial_coords\n",
    "adata1.obs['x_pos'] = spatial_coords[:, 0]\n",
    "adata1.obs['y_pos'] = spatial_coords[:, 1]\n",
    "\n",
    "# Modality 2: Lower variance genes\n",
    "adata2 = sc.AnnData(data2, dtype='float64')\n",
    "adata2.obs['Layertype'] = layer_annotations\n",
    "adata2.obsm['spatial'] = spatial_coords\n",
    "adata2.obs['x_pos'] = spatial_coords[:, 0]\n",
    "adata2.obs['y_pos'] = spatial_coords[:, 1]\n",
    "\n",
    "print(f\"\\nModality 1 shape: {adata1.shape}\")\n",
    "print(f\"Modality 2 shape: {adata2.shape}\")\n",
    "print(f\"\\n✓ Data split into two modalities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Assess Data Complexity\n",
    "\n",
    "Let's determine if this is complex (like MouseBrain) or simple (like VisualCortex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute spatial autocorrelation (Moran's I)\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def compute_spatial_autocorrelation(coords, values, k=10):\n",
    "    \"\"\"Compute Moran's I for spatial autocorrelation\"\"\"\n",
    "    # Build spatial weights matrix\n",
    "    W = kneighbors_graph(coords, k, mode='connectivity')\n",
    "    W = W.toarray()\n",
    "    \n",
    "    # Normalize\n",
    "    W = W / W.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    # Compute Moran's I\n",
    "    n = len(values)\n",
    "    mean_val = values.mean()\n",
    "    \n",
    "    numerator = 0\n",
    "    denominator = ((values - mean_val) ** 2).sum()\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            numerator += W[i, j] * (values[i] - mean_val) * (values[j] - mean_val)\n",
    "    \n",
    "    moran_i = (n / W.sum()) * (numerator / denominator)\n",
    "    return moran_i\n",
    "\n",
    "# Encode layers as numeric\n",
    "layer_numeric = pd.Categorical(layer_annotations).codes\n",
    "moran_i = compute_spatial_autocorrelation(spatial_coords, layer_numeric)\n",
    "\n",
    "print(f\"Spatial Autocorrelation (Moran's I): {moran_i:.3f}\")\n",
    "print(f\"\\nInterpretation:\")\n",
    "if moran_i > 0.7:\n",
    "    print(\"  → HIGH spatial structure (layered, like VisualCortex)\")\n",
    "    print(\"  → Recommendation: GCN may perform better\")\n",
    "    print(\"  → For GT+PE: use num_heads=2, dropout=0.3, use_pe=False\")\n",
    "    complexity = \"simple\"\n",
    "elif moran_i > 0.4:\n",
    "    print(\"  → MODERATE spatial structure\")\n",
    "    print(\"  → Recommendation: Test both GCN and GT+PE\")\n",
    "    complexity = \"moderate\"\n",
    "else:\n",
    "    print(\"  → LOW spatial structure (complex, like MouseBrain)\")\n",
    "    print(\"  → Recommendation: GT+PE likely to perform better\")\n",
    "    print(\"  → For GT+PE: use num_heads=8, dropout=0.1, use_pe=True\")\n",
    "    complexity = \"complex\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run COSMOS with GCN (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import original COSMOS with GCN\n",
    "from COSMOS.cosmos import Cosmos as Cosmos_GCN\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING: GCN (Original COSMOS)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create COSMOS model\n",
    "cosmos_gcn = Cosmos_GCN(adata1=adata1, adata2=adata2)\n",
    "cosmos_gcn.preprocessing_data(n_neighbors=10)\n",
    "\n",
    "# Train\n",
    "embedding_gcn = cosmos_gcn.train(\n",
    "    spatial_regularization_strength=0.01,\n",
    "    z_dim=50,\n",
    "    lr=1e-3,\n",
    "    wnn_epoch=100,  # Compute WNN earlier\n",
    "    total_epoch=1000,\n",
    "    max_patience_bef=10,\n",
    "    max_patience_aft=30,\n",
    "    min_stop=200,\n",
    "    random_seed=random_seed,\n",
    "    gpu=0,\n",
    "    regularization_acceleration=True,\n",
    "    edge_subset_sz=1000000\n",
    ")\n",
    "\n",
    "weights_gcn = cosmos_gcn.weights\n",
    "df_embedding_gcn = pd.DataFrame(embedding_gcn)\n",
    "\n",
    "print(f\"\\n✓ GCN training complete\")\n",
    "print(f\"✓ Embedding shape: {df_embedding_gcn.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run COSMOS with Graph Transformer + PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Graph Transformer version\n",
    "from COSMOS.cosmos_transformer_pe_version import Cosmos as Cosmos_GT_PE\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING: Graph Transformer + Positional Encoding\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Set hyperparameters based on data complexity\n",
    "if complexity == \"simple\":\n",
    "    # For layered/simple data\n",
    "    num_heads = 2\n",
    "    dropout = 0.3\n",
    "    use_pe = False\n",
    "    pe_dim = 0\n",
    "    print(\"Using SIMPLE data parameters (no PE, fewer heads)\")\n",
    "elif complexity == \"complex\":\n",
    "    # For complex/heterogeneous data\n",
    "    num_heads = 8\n",
    "    dropout = 0.1\n",
    "    use_pe = True\n",
    "    pe_dim = 8\n",
    "    print(\"Using COMPLEX data parameters (with PE, more heads)\")\n",
    "else:\n",
    "    # Moderate - try with PE\n",
    "    num_heads = 4\n",
    "    dropout = 0.2\n",
    "    use_pe = True\n",
    "    pe_dim = 8\n",
    "    print(\"Using MODERATE data parameters\")\n",
    "\n",
    "# Create COSMOS model\n",
    "cosmos_gt_pe = Cosmos_GT_PE(adata1=adata1, adata2=adata2)\n",
    "cosmos_gt_pe.preprocessing_data(n_neighbors=10)\n",
    "\n",
    "# Train\n",
    "embedding_gt_pe = cosmos_gt_pe.train(\n",
    "    spatial_regularization_strength=0.01,\n",
    "    z_dim=50,\n",
    "    lr=1e-3,\n",
    "    wnn_epoch=100,  # Compute WNN earlier\n",
    "    total_epoch=1000,\n",
    "    max_patience_bef=10,\n",
    "    max_patience_aft=30,\n",
    "    min_stop=200,\n",
    "    random_seed=random_seed,\n",
    "    gpu=0,\n",
    "    regularization_acceleration=True,\n",
    "    edge_subset_sz=1000000,\n",
    "    # Graph Transformer parameters\n",
    "    num_heads=num_heads,\n",
    "    dropout=dropout,\n",
    "    pe_dim=pe_dim,\n",
    "    use_pe=use_pe\n",
    ")\n",
    "\n",
    "weights_gt_pe = cosmos_gt_pe.weights\n",
    "df_embedding_gt_pe = pd.DataFrame(embedding_gt_pe)\n",
    "\n",
    "print(f\"\\n✓ GT+PE training complete\")\n",
    "print(f\"✓ Embedding shape: {df_embedding_gt_pe.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Clustering and Evaluation - GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create AnnData for clustering (GCN)\n",
    "adata_result_gcn = adata1.copy()\n",
    "embedding_adata_gcn = sc.AnnData(df_embedding_gcn)\n",
    "\n",
    "# Compute neighbors\n",
    "sc.pp.neighbors(embedding_adata_gcn, n_neighbors=50, use_rep='X')\n",
    "\n",
    "# Find optimal clustering resolution\n",
    "best_ari_gcn = 0\n",
    "best_res_gcn = 0.5\n",
    "\n",
    "for res in [0.3, 0.5, 0.8, 1.0, 1.2, 1.5, 2.0]:\n",
    "    sc.tl.louvain(embedding_adata_gcn, resolution=res)\n",
    "    clusters = embedding_adata_gcn.obs['louvain']\n",
    "    ari = sklearn.metrics.adjusted_rand_score(layer_annotations, clusters)\n",
    "    if ari > best_ari_gcn:\n",
    "        best_ari_gcn = ari\n",
    "        best_res_gcn = res\n",
    "\n",
    "print(f\"GCN - Best ARI: {best_ari_gcn:.3f} at resolution {best_res_gcn}\")\n",
    "\n",
    "# Cluster with best resolution\n",
    "sc.tl.louvain(embedding_adata_gcn, resolution=best_res_gcn)\n",
    "adata_result_gcn.obs['Cluster_gcn'] = list(embedding_adata_gcn.obs['louvain'])\n",
    "adata_result_gcn.obs['Cluster_gcn'] = adata_result_gcn.obs['Cluster_gcn'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Clustering and Evaluation - GT+PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create AnnData for clustering (GT+PE)\n",
    "adata_result_gt_pe = adata1.copy()\n",
    "embedding_adata_gt_pe = sc.AnnData(df_embedding_gt_pe)\n",
    "\n",
    "# Compute neighbors\n",
    "sc.pp.neighbors(embedding_adata_gt_pe, n_neighbors=50, use_rep='X')\n",
    "\n",
    "# Find optimal clustering resolution\n",
    "best_ari_gt_pe = 0\n",
    "best_res_gt_pe = 0.5\n",
    "\n",
    "for res in [0.3, 0.5, 0.8, 1.0, 1.2, 1.5, 2.0]:\n",
    "    sc.tl.louvain(embedding_adata_gt_pe, resolution=res)\n",
    "    clusters = embedding_adata_gt_pe.obs['louvain']\n",
    "    ari = sklearn.metrics.adjusted_rand_score(layer_annotations, clusters)\n",
    "    if ari > best_ari_gt_pe:\n",
    "        best_ari_gt_pe = ari\n",
    "        best_res_gt_pe = res\n",
    "\n",
    "print(f\"GT+PE - Best ARI: {best_ari_gt_pe:.3f} at resolution {best_res_gt_pe}\")\n",
    "\n",
    "# Cluster with best resolution\n",
    "sc.tl.louvain(embedding_adata_gt_pe, resolution=best_res_gt_pe)\n",
    "adata_result_gt_pe.obs['Cluster_gt_pe'] = list(embedding_adata_gt_pe.obs['louvain'])\n",
    "adata_result_gt_pe.obs['Cluster_gt_pe'] = adata_result_gt_pe.obs['Cluster_gt_pe'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualization: Spatial Clustering Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison\n",
    "matplotlib.rcParams['font.size'] = 10\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "plot_colors = ['#D1D1D1', '#e6194b', '#3cb44b', '#ffe119', '#4363d8', '#f58231', \n",
    "               '#911eb4', '#46f0f0', '#f032e6', '#bcf60c', '#fabebe', '#008080',\n",
    "               '#e6beff', '#9a6324', '#ffd8b1', '#800000', '#aaffc3', '#808000']\n",
    "\n",
    "# Ground truth\n",
    "ax = axes[0, 0]\n",
    "for i, layer in enumerate(np.unique(layer_annotations)):\n",
    "    mask = layer_annotations == layer\n",
    "    ax.scatter(spatial_coords[mask, 0], spatial_coords[mask, 1],\n",
    "               c=plot_colors[i], label=layer, s=10, alpha=0.8)\n",
    "ax.set_title('Manual Annotation', fontweight='bold')\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', frameon=False, fontsize=8)\n",
    "ax.axis('equal')\n",
    "ax.axis('off')\n",
    "\n",
    "# GCN results\n",
    "ax = axes[0, 1]\n",
    "clusters_gcn = adata_result_gcn.obs['Cluster_gcn'].values\n",
    "for i, cluster in enumerate(np.unique(clusters_gcn)):\n",
    "    mask = clusters_gcn == cluster\n",
    "    ax.scatter(spatial_coords[mask, 0], spatial_coords[mask, 1],\n",
    "               c=plot_colors[i % len(plot_colors)], label=str(cluster), s=10, alpha=0.8)\n",
    "ax.set_title(f'GCN (Original), ARI = {best_ari_gcn:.2f}', fontweight='bold')\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', frameon=False, fontsize=8)\n",
    "ax.axis('equal')\n",
    "ax.axis('off')\n",
    "\n",
    "# GT+PE results\n",
    "ax = axes[1, 0]\n",
    "clusters_gt_pe = adata_result_gt_pe.obs['Cluster_gt_pe'].values\n",
    "for i, cluster in enumerate(np.unique(clusters_gt_pe)):\n",
    "    mask = clusters_gt_pe == cluster\n",
    "    ax.scatter(spatial_coords[mask, 0], spatial_coords[mask, 1],\n",
    "               c=plot_colors[i % len(plot_colors)], label=str(cluster), s=10, alpha=0.8)\n",
    "ax.set_title(f'GT+PE, ARI = {best_ari_gt_pe:.2f}', fontweight='bold')\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', frameon=False, fontsize=8)\n",
    "ax.axis('equal')\n",
    "ax.axis('off')\n",
    "\n",
    "# Performance comparison\n",
    "ax = axes[1, 1]\n",
    "methods = ['GCN', 'GT+PE']\n",
    "aris = [best_ari_gcn, best_ari_gt_pe]\n",
    "colors_bar = ['#3cb44b' if aris[0] > aris[1] else '#e6194b',\n",
    "              '#e6194b' if aris[1] > aris[0] else '#3cb44b']\n",
    "bars = ax.bar(methods, aris, color=colors_bar, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "ax.set_ylabel('Adjusted Rand Index (ARI)', fontweight='bold')\n",
    "ax.set_title('Performance Comparison', fontweight='bold')\n",
    "ax.set_ylim([0, 1])\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, ari in zip(bars, aris):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{ari:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('MOB_comparison_GCN_vs_GT_PE.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL RESULTS: MOB Dataset\")\n",
    "print(\"=\"*60)\n",
    "print(f\"GCN (Original):         ARI = {best_ari_gcn:.3f}\")\n",
    "print(f\"GT+PE (Enhanced):       ARI = {best_ari_gt_pe:.3f}\")\n",
    "print(f\"Improvement:            {((best_ari_gt_pe - best_ari_gcn) / best_ari_gcn * 100):+.1f}%\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. UMAP Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP for GCN\n",
    "umap_2d_gcn = UMAP(n_components=2, init='random', random_state=random_seed, \n",
    "                    min_dist=0.3, n_neighbors=30)\n",
    "umap_pos_gcn = umap_2d_gcn.fit_transform(df_embedding_gcn)\n",
    "adata_result_gcn.obs['umap_x'] = umap_pos_gcn[:, 0]\n",
    "adata_result_gcn.obs['umap_y'] = umap_pos_gcn[:, 1]\n",
    "\n",
    "# UMAP for GT+PE\n",
    "umap_2d_gt_pe = UMAP(n_components=2, init='random', random_state=random_seed,\n",
    "                      min_dist=0.3, n_neighbors=30)\n",
    "umap_pos_gt_pe = umap_2d_gt_pe.fit_transform(df_embedding_gt_pe)\n",
    "adata_result_gt_pe.obs['umap_x'] = umap_pos_gt_pe[:, 0]\n",
    "adata_result_gt_pe.obs['umap_y'] = umap_pos_gt_pe[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot UMAP comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# UMAP - GCN\n",
    "ax = axes[0]\n",
    "clusters_gcn = adata_result_gcn.obs['Cluster_gcn'].values\n",
    "for i, cluster in enumerate(np.unique(clusters_gcn)):\n",
    "    mask = clusters_gcn == cluster\n",
    "    ax.scatter(adata_result_gcn.obs['umap_x'][mask], \n",
    "               adata_result_gcn.obs['umap_y'][mask],\n",
    "               c=plot_colors[i % len(plot_colors)], label=str(cluster), \n",
    "               s=5, alpha=0.6)\n",
    "ax.set_title(f'UMAP: GCN (ARI = {best_ari_gcn:.2f})', fontweight='bold')\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', frameon=False, fontsize=8)\n",
    "ax.axis('off')\n",
    "\n",
    "# UMAP - GT+PE\n",
    "ax = axes[1]\n",
    "clusters_gt_pe = adata_result_gt_pe.obs['Cluster_gt_pe'].values\n",
    "for i, cluster in enumerate(np.unique(clusters_gt_pe)):\n",
    "    mask = clusters_gt_pe == cluster\n",
    "    ax.scatter(adata_result_gt_pe.obs['umap_x'][mask],\n",
    "               adata_result_gt_pe.obs['umap_y'][mask],\n",
    "               c=plot_colors[i % len(plot_colors)], label=str(cluster),\n",
    "               s=5, alpha=0.6)\n",
    "ax.set_title(f'UMAP: GT+PE (ARI = {best_ari_gt_pe:.2f})', fontweight='bold')\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', frameon=False, fontsize=8)\n",
    "ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('MOB_UMAP_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Modality Weight Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weight_analysis(weights, layer_labels, title_suffix):\n",
    "    \"\"\"Plot modality weight distribution by layer\"\"\"\n",
    "    df = pd.DataFrame({\n",
    "        'Modality_1': weights[:, 0],\n",
    "        'Modality_2': weights[:, 1],\n",
    "        'Layer': layer_labels\n",
    "    })\n",
    "    \n",
    "    df_melted = df.melt(id_vars=['Layer'], var_name='Modality', value_name='Weight')\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "    sns.violinplot(data=df_melted, x='Layer', y='Weight', hue='Modality',\n",
    "                   split=True, inner='quart', ax=ax)\n",
    "    ax.set_title(f'Modality Weights by Layer - {title_suffix}', fontweight='bold')\n",
    "    ax.set_xlabel('Layer', fontweight='bold')\n",
    "    ax.set_ylabel('Weight', fontweight='bold')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"\\n{title_suffix} - Modality Weight Statistics:\")\n",
    "    print(f\"  Modality 1: mean={weights[:, 0].mean():.3f}, std={weights[:, 0].std():.3f}\")\n",
    "    print(f\"  Modality 2: mean={weights[:, 1].mean():.3f}, std={weights[:, 1].std():.3f}\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Modality Weight Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "plot_weight_analysis(weights_gcn, layer_annotations, \"GCN\")\n",
    "plot_weight_analysis(weights_gt_pe, layer_annotations, \"GT+PE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL SUMMARY: MOB Dataset Analysis\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nDataset Characteristics:\")\n",
    "print(f\"  - Cells: {adata1.shape[0]}\")\n",
    "print(f\"  - Modality 1 features: {adata1.shape[1]}\")\n",
    "print(f\"  - Modality 2 features: {adata2.shape[1]}\")\n",
    "print(f\"  - Layers: {len(np.unique(layer_annotations))}\")\n",
    "print(f\"  - Spatial autocorrelation: {moran_i:.3f} ({complexity})\")\n",
    "\n",
    "print(f\"\\nPerformance Results:\")\n",
    "print(f\"  GCN (Original):         ARI = {best_ari_gcn:.3f}\")\n",
    "print(f\"  GT+PE (Enhanced):       ARI = {best_ari_gt_pe:.3f}\")\n",
    "\n",
    "improvement = ((best_ari_gt_pe - best_ari_gcn) / best_ari_gcn * 100) if best_ari_gcn > 0 else 0\n",
    "print(f\"  Relative improvement:   {improvement:+.1f}%\")\n",
    "\n",
    "if best_ari_gt_pe > best_ari_gcn:\n",
    "    print(f\"\\n✓ GT+PE outperforms GCN by {improvement:.1f}%\")\n",
    "    print(f\"  → This dataset benefits from global attention and topology awareness\")\n",
    "elif best_ari_gcn > best_ari_gt_pe:\n",
    "    print(f\"\\n✓ GCN outperforms GT+PE by {-improvement:.1f}%\")\n",
    "    print(f\"  → This dataset has strong local structure (like VisualCortex)\")\n",
    "    print(f\"  → Consider using GCN for production\")\n",
    "else:\n",
    "    print(f\"\\n≈ Both methods perform similarly\")\n",
    "    print(f\"  → Choose based on computational constraints\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Analysis complete! Results saved to:\")\n",
    "print(\"  - MOB_comparison_GCN_vs_GT_PE.png\")\n",
    "print(\"  - MOB_UMAP_comparison.png\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
