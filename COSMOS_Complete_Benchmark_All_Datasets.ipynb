{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# COSMOS Comprehensive Benchmark: GCN vs Graph Transformer + PE\n",
        "\n",
        "## Complete Analysis for All 4 Datasets\n",
        "\n",
        "**Datasets:**\n",
        "- D1: Mouse Brain (ATAC + RNA)\n",
        "- D2: Mouse Visual Cortex (Simulation)\n",
        "- D3: Mouse Olfactory Bulb (MOB)\n",
        "\n",
        "**Metrics (from COSMOS paper):**\n",
        "- ARI (Adjusted Rand Index)\n",
        "- NMI (Normalized Mutual Information)\n",
        "- Silhouette Score\n",
        "- Homogeneity & Completeness\n",
        "- Spatial Coherence\n",
        "- Pseudo-spatial Metrics (pSM) for trajectory data\n",
        "\n",
        "**Architecture:**\n",
        "- GCN (Original COSMOS)\n",
        "- Graph Transformer + PE (Enhanced)\n",
        "\n",
        "**Manual Control:**\n",
        "All model hyperparameters are in Section 2 for easy modification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1. Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scanpy as sc\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from umap import UMAP\n",
        "import sklearn\n",
        "import sklearn.metrics\n",
        "from sklearn.metrics import (\n",
        "    adjusted_rand_score, \n",
        "    normalized_mutual_info_score,\n",
        "    silhouette_score,\n",
        "    homogeneity_score,\n",
        "    completeness_score,\n",
        "    v_measure_score\n",
        ")\n",
        "import seaborn as sns\n",
        "import h5py\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import COSMOS\n",
        "from COSMOS.cosmos import Cosmos as Cosmos_GCN\n",
        "from COSMOS.cosmos_transformer_pe_version import Cosmos as Cosmos_GT_PE\n",
        "\n",
        "print(\"‚úì All libraries imported successfully\")\n",
        "print(f\"  - scanpy version: {sc.__version__}\")\n",
        "print(f\"  - numpy version: {np.__version__}\")\n",
        "print(f\"  - pandas version: {pd.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2. HYPERPARAMETER CONFIGURATION\n",
        "\n",
        "**üéõÔ∏è MODIFY THESE PARAMETERS AS NEEDED**\n",
        "\n",
        "All model-related hyperparameters are here for easy access."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# MANUAL HYPERPARAMETER CONFIGURATION\n",
        "# ============================================================\n",
        "\n",
        "# ------------------------------\n",
        "# Dataset Selection\n",
        "# ------------------------------\n",
        "# Choose which dataset to run: 'D1', 'D2', 'D3', 'D4', 'D5' or 'ALL'\n",
        "DATASET_TO_RUN = 'D1'  # Change this to test specific datasets\n",
        "\n",
        "# ------------------------------\n",
        "# General Training Parameters\n",
        "# ------------------------------\n",
        "RANDOM_SEED = 42\n",
        "GPU_ID = 0\n",
        "LEARNING_RATE = 1e-3\n",
        "TOTAL_EPOCHS = 1000\n",
        "WNN_EPOCH = 100  # When to compute WNN weights\n",
        "MAX_PATIENCE_BEF = 10\n",
        "MAX_PATIENCE_AFT = 30\n",
        "MIN_STOP = 200\n",
        "SPATIAL_REG_STRENGTH = 0.01\n",
        "REGULARIZATION_ACCELERATION = True\n",
        "EDGE_SUBSET_SIZE = 1000000\n",
        "\n",
        "# ------------------------------\n",
        "# Graph Construction\n",
        "# ------------------------------\n",
        "N_NEIGHBORS = 10  # For spatial graph construction\n",
        "\n",
        "# ------------------------------\n",
        "# GCN Hyperparameters\n",
        "# ------------------------------\n",
        "GCN_Z_DIM = 50  # Output embedding dimension\n",
        "\n",
        "# ------------------------------\n",
        "# Graph Transformer + PE Hyperparameters\n",
        "# ------------------------------\n",
        "# For D1 (MouseBrain) - Complex data\n",
        "GT_PE_D1_CONFIG = {\n",
        "    'z_dim': 50,\n",
        "    'num_heads': 8,      # Number of attention heads\n",
        "    'dropout': 0.1,      # Dropout rate\n",
        "    'pe_dim': 8,         # Positional encoding dimension\n",
        "    'use_pe': True       # Enable/disable PE\n",
        "}\n",
        "\n",
        "# For D2 (VisualCortex) - Layered data\n",
        "GT_PE_D2_CONFIG = {\n",
        "    'z_dim': 50,\n",
        "    'num_heads': 2,      # Fewer heads for simple data\n",
        "    'dropout': 0.3,      # Higher dropout\n",
        "    'pe_dim': 0,         # No PE for layered data\n",
        "    'use_pe': False      # Disable PE\n",
        "}\n",
        "\n",
        "# For D3 (MOB) - Test both configurations\n",
        "GT_PE_D3_CONFIG = {\n",
        "    'z_dim': 50,\n",
        "    'num_heads': 8,      # Start with complex settings\n",
        "    'dropout': 0.1,\n",
        "    'pe_dim': 8,\n",
        "    'use_pe': True\n",
        "}\n",
        "\n",
        "\n",
        "# For D4 (SpatialGlue Simulation) - Ground truth factor data\n",
        "GT_PE_D4_CONFIG = {\n",
        "    'z_dim': 50,\n",
        "    'num_heads': 4,      # Moderate heads for simulated data\n",
        "    'dropout': 0.2,      # Moderate dropout\n",
        "    'pe_dim': 8,         # Use PE to capture spatial patterns\n",
        "    'use_pe': True\n",
        "}\n",
        "\n",
        "\n",
        "# For D5 (Spatial Multiomics Multi-Simulated) - Complex multi-domain data\n",
        "GT_PE_D5_CONFIG = {\n",
        "    'z_dim': 50,\n",
        "    'num_heads': 8,      # More heads for complex domain structure\n",
        "    'dropout': 0.1,      # Lower dropout for larger dataset (3000 cells)\n",
        "    'pe_dim': 16,        # Larger PE for bigger spatial layout\n",
        "    'use_pe': True       # Enable PE for spatial awareness\n",
        "}\n",
        "\n",
        "# ------------------------------\n",
        "# Evaluation Parameters (Auto-tuned)\n",
        "# ------------------------------\n",
        "# These will be automatically optimized during evaluation\n",
        "# CLUSTERING_RESOLUTIONS = [0.3, 0.5, 0.8, 1.0, 1.2, 1.5, 2.0]  # Test range\n",
        "CLUSTERING_RESOLUTIONS = [1.0]\n",
        "UMAP_N_NEIGHBORS = 30\n",
        "UMAP_MIN_DIST = 0.3\n",
        "NEIGHBORS_FOR_CLUSTERING = 50\n",
        "\n",
        "# ------------------------------\n",
        "# Visualization Parameters\n",
        "# ------------------------------\n",
        "FIGURE_DPI = 300\n",
        "POINT_SIZE_SPATIAL = 10\n",
        "POINT_SIZE_UMAP = 5\n",
        "FONT_SIZE = 10\n",
        "\n",
        "# ============================================================\n",
        "# END OF CONFIGURATION\n",
        "# ============================================================\n",
        "\n",
        "print(\"‚úì Hyperparameters configured\")\n",
        "print(f\"  - Dataset to run: {DATASET_TO_RUN}\")\n",
        "print(f\"  - Random seed: {RANDOM_SEED}\")\n",
        "print(f\"  - Total epochs: {TOTAL_EPOCHS}\")\n",
        "print(f\"  - WNN epoch: {WNN_EPOCH}\")\n",
        "print(f\"\\n‚úì Configuration complete - ready to run!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3. Helper Functions for Metrics and Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set random seeds\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "def compute_all_metrics(true_labels, predicted_labels, embeddings):\n",
        "    \"\"\"\n",
        "    Compute comprehensive metrics as used in COSMOS paper.\n",
        "    \n",
        "    Parameters:\n",
        "    - true_labels: Ground truth cluster labels\n",
        "    - predicted_labels: Predicted cluster labels\n",
        "    - embeddings: Cell embeddings\n",
        "    \n",
        "    Returns:\n",
        "    - Dictionary of metrics\n",
        "    \"\"\"\n",
        "    metrics = {}\n",
        "    \n",
        "    # Clustering accuracy metrics\n",
        "    metrics['ARI'] = adjusted_rand_score(true_labels, predicted_labels)\n",
        "    metrics['NMI'] = normalized_mutual_info_score(true_labels, predicted_labels)\n",
        "    metrics['Homogeneity'] = homogeneity_score(true_labels, predicted_labels)\n",
        "    metrics['Completeness'] = completeness_score(true_labels, predicted_labels)\n",
        "    metrics['V-measure'] = v_measure_score(true_labels, predicted_labels)\n",
        "    \n",
        "    # Embedding quality\n",
        "    try:\n",
        "        # Silhouette score (can be slow for large datasets)\n",
        "        if len(embeddings) < 5000:\n",
        "            metrics['Silhouette'] = silhouette_score(embeddings, true_labels)\n",
        "        else:\n",
        "            # Sample for efficiency\n",
        "            sample_idx = np.random.choice(len(embeddings), 5000, replace=False)\n",
        "            metrics['Silhouette'] = silhouette_score(\n",
        "                embeddings[sample_idx], \n",
        "                true_labels[sample_idx]\n",
        "            )\n",
        "    except:\n",
        "        metrics['Silhouette'] = np.nan\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "\n",
        "def compute_spatial_coherence(spatial_coords, cluster_labels, k=10):\n",
        "    \"\"\"\n",
        "    Compute spatial coherence: fraction of neighbors with same cluster label.\n",
        "    Higher = better spatial organization.\n",
        "    \"\"\"\n",
        "    from sklearn.neighbors import NearestNeighbors\n",
        "    \n",
        "    # Find k nearest neighbors\n",
        "    nbrs = NearestNeighbors(n_neighbors=k+1).fit(spatial_coords)\n",
        "    distances, indices = nbrs.kneighbors(spatial_coords)\n",
        "    \n",
        "    # Compute coherence for each cell\n",
        "    coherence_scores = []\n",
        "    for i in range(len(cluster_labels)):\n",
        "        cell_label = cluster_labels[i]\n",
        "        neighbor_labels = cluster_labels[indices[i, 1:]]  # Exclude self\n",
        "        coherence = (neighbor_labels == cell_label).sum() / k\n",
        "        coherence_scores.append(coherence)\n",
        "    \n",
        "    return np.mean(coherence_scores)\n",
        "\n",
        "\n",
        "def find_optimal_clustering(embedding_adata, true_labels, resolutions):\n",
        "    \"\"\"\n",
        "    Find optimal clustering resolution by maximizing ARI.\n",
        "    This is auto-tuned (not a model parameter).\n",
        "    \"\"\"\n",
        "    best_ari = 0\n",
        "    best_resolution = resolutions[0]\n",
        "    best_clusters = None\n",
        "    \n",
        "    for res in resolutions:\n",
        "        sc.tl.louvain(embedding_adata, resolution=res)\n",
        "        clusters = embedding_adata.obs['louvain'].values\n",
        "        ari = adjusted_rand_score(true_labels, clusters)\n",
        "        \n",
        "        if ari > best_ari:\n",
        "            best_ari = ari\n",
        "            best_resolution = res\n",
        "            best_clusters = clusters.copy()\n",
        "    \n",
        "    return best_clusters, best_resolution, best_ari\n",
        "\n",
        "\n",
        "def print_metrics_table(metrics_dict, method_name):\n",
        "    \"\"\"\n",
        "    Print metrics in a formatted table.\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"{method_name} - Performance Metrics\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"{'Metric':<20} {'Value':>10}\")\n",
        "    print(f\"{'-'*60}\")\n",
        "    \n",
        "    for metric, value in metrics_dict.items():\n",
        "        if isinstance(value, float):\n",
        "            print(f\"{metric:<20} {value:>10.4f}\")\n",
        "        else:\n",
        "            print(f\"{metric:<20} {value:>10}\")\n",
        "    \n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "\n",
        "def create_comparison_plot(adata, spatial_coords, true_labels, \n",
        "                           gcn_clusters, gt_pe_clusters,\n",
        "                           gcn_ari, gt_pe_ari, dataset_name):\n",
        "    \"\"\"\n",
        "    Create comprehensive comparison figure.\n",
        "    \"\"\"\n",
        "    matplotlib.rcParams['font.size'] = FONT_SIZE\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 11))\n",
        "    \n",
        "    plot_colors = ['#D1D1D1', '#e6194b', '#3cb44b', '#ffe119', '#4363d8', \n",
        "                   '#f58231', '#911eb4', '#46f0f0', '#f032e6', '#bcf60c', \n",
        "                   '#fabebe', '#008080', '#e6beff', '#9a6324', '#ffd8b1', \n",
        "                   '#800000', '#aaffc3', '#808000', '#000075', '#808080']\n",
        "    \n",
        "    # Plot 1: Ground Truth\n",
        "    ax = axes[0, 0]\n",
        "    unique_labels = np.unique(true_labels)\n",
        "    for i, label in enumerate(unique_labels):\n",
        "        mask = true_labels == label\n",
        "        ax.scatter(spatial_coords[mask, 0], spatial_coords[mask, 1],\n",
        "                   c=plot_colors[i % len(plot_colors)], label=str(label),\n",
        "                   s=POINT_SIZE_SPATIAL, alpha=0.8)\n",
        "    ax.set_title('Ground Truth Annotation', fontweight='bold', fontsize=12)\n",
        "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', frameon=False, fontsize=8)\n",
        "    ax.axis('equal')\n",
        "    ax.axis('off')\n",
        "    \n",
        "    # Plot 2: GCN Results\n",
        "    ax = axes[0, 1]\n",
        "    unique_clusters = np.unique(gcn_clusters)\n",
        "    for i, cluster in enumerate(unique_clusters):\n",
        "        mask = gcn_clusters == cluster\n",
        "        ax.scatter(spatial_coords[mask, 0], spatial_coords[mask, 1],\n",
        "                   c=plot_colors[i % len(plot_colors)], label=str(cluster),\n",
        "                   s=POINT_SIZE_SPATIAL, alpha=0.8)\n",
        "    ax.set_title(f'GCN (Original)\\nARI = {gcn_ari:.3f}', \n",
        "                 fontweight='bold', fontsize=12)\n",
        "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', frameon=False, fontsize=8)\n",
        "    ax.axis('equal')\n",
        "    ax.axis('off')\n",
        "    \n",
        "    # Plot 3: GT+PE Results\n",
        "    ax = axes[1, 0]\n",
        "    unique_clusters = np.unique(gt_pe_clusters)\n",
        "    for i, cluster in enumerate(unique_clusters):\n",
        "        mask = gt_pe_clusters == cluster\n",
        "        ax.scatter(spatial_coords[mask, 0], spatial_coords[mask, 1],\n",
        "                   c=plot_colors[i % len(plot_colors)], label=str(cluster),\n",
        "                   s=POINT_SIZE_SPATIAL, alpha=0.8)\n",
        "    ax.set_title(f'Graph Transformer + PE\\nARI = {gt_pe_ari:.3f}', \n",
        "                 fontweight='bold', fontsize=12)\n",
        "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', frameon=False, fontsize=8)\n",
        "    ax.axis('equal')\n",
        "    ax.axis('off')\n",
        "    \n",
        "    # Plot 4: Performance Comparison\n",
        "    ax = axes[1, 1]\n",
        "    methods = ['GCN', 'GT+PE']\n",
        "    aris = [gcn_ari, gt_pe_ari]\n",
        "    colors_bar = ['#3cb44b' if gcn_ari < gt_pe_ari else '#e6194b',\n",
        "                  '#e6194b' if gt_pe_ari < gcn_ari else '#3cb44b']\n",
        "    bars = ax.bar(methods, aris, color=colors_bar, alpha=0.7, \n",
        "                   edgecolor='black', linewidth=2)\n",
        "    ax.set_ylabel('Adjusted Rand Index (ARI)', fontweight='bold')\n",
        "    ax.set_title('Performance Comparison', fontweight='bold', fontsize=12)\n",
        "    ax.set_ylim([0, 1])\n",
        "    ax.grid(axis='y', alpha=0.3)\n",
        "    \n",
        "    for bar, ari in zip(bars, aris):\n",
        "        height = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                f'{ari:.3f}', ha='center', va='bottom', \n",
        "                fontweight='bold', fontsize=12)\n",
        "    \n",
        "    plt.suptitle(f'{dataset_name} - Spatial Clustering Comparison', \n",
        "                 fontsize=14, fontweight='bold', y=0.995)\n",
        "    plt.tight_layout()\n",
        "    \n",
        "    return fig\n",
        "\n",
        "\n",
        "print(\"‚úì Helper functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4. Dataset Loading Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_mousebrain_data():\n",
        "    \"\"\"\n",
        "    Load D1: Mouse Brain ATAC + RNA dataset.\n",
        "    \"\"\"\n",
        "    print(\"\\nLoading D1: Mouse Brain (ATAC + RNA)...\")\n",
        "    \n",
        "    data_mat = h5py.File('./datasets/ATAC_RNA_Seq_MouseBrain_RNA_ATAC.h5', 'r')\n",
        "    df_data_RNA = np.array(data_mat['X_RNA']).astype('float64')\n",
        "    df_data_ATAC = np.array(data_mat['X_ATAC']).astype('float64')\n",
        "    loc = np.array(data_mat['Pos']).astype('float64')\n",
        "    LayerName = [item.decode(\"utf-8\") for item in list(data_mat['LayerName'])]\n",
        "    \n",
        "    adata1 = sc.AnnData(df_data_RNA, dtype=\"float64\")\n",
        "    adata1.obsm['spatial'] = np.array(loc)\n",
        "    adata1.obs['LayerName'] = LayerName\n",
        "    adata1.obs['x_pos'] = np.array(loc)[:, 0]\n",
        "    adata1.obs['y_pos'] = np.array(loc)[:, 1]\n",
        "    \n",
        "    adata2 = sc.AnnData(df_data_ATAC, dtype=\"float64\")\n",
        "    adata2.obsm['spatial'] = np.array(loc)\n",
        "    adata2.obs['LayerName'] = LayerName\n",
        "    adata2.obs['x_pos'] = np.array(loc)[:, 0]\n",
        "    adata2.obs['y_pos'] = np.array(loc)[:, 1]\n",
        "    \n",
        "    print(f\"  - Cells: {adata1.shape[0]}\")\n",
        "    print(f\"  - RNA features: {adata1.shape[1]}\")\n",
        "    print(f\"  - ATAC features: {adata2.shape[1]}\")\n",
        "    print(f\"  - Layers: {len(np.unique(LayerName))}\")\n",
        "    \n",
        "    return adata1, adata2, np.array(LayerName), loc, 'MouseBrain'\n",
        "\n",
        "\n",
        "def load_visualcortex_data():\n",
        "    \"\"\"\n",
        "    Load D2: Mouse Visual Cortex simulation dataset from CSV files.\n",
        "    Generates two views (adata1, adata2) via synthetic noise and shuffling.\n",
        "    \"\"\"\n",
        "    print(\"\\nLoading D2: Mouse Visual Cortex (Simulation)...\")\n",
        "    \n",
        "    # 1. Load Data from CSVs\n",
        "    df_data = pd.read_csv('./datasets/MVC_counts.csv', sep=\",\", header=0, na_filter=False, index_col=0) \n",
        "    df_meta = pd.read_csv('./datasets/MVC_meta.csv', sep=\",\", header=0, na_filter=False, index_col=0) \n",
        "    \n",
        "    # 2. Process Metadata and Spatial info\n",
        "    df_pixels = df_meta.iloc[:, 2:4]\n",
        "    df_labels = list(df_meta.iloc[:, 1])\n",
        "    \n",
        "    # Create base AnnData\n",
        "    adata = sc.AnnData(X=df_data)\n",
        "    adata.obs['LayerName'] = df_labels\n",
        "    adata.obs['LayerName_2'] = list(df_meta.iloc[:, 4])\n",
        "\n",
        "    # Spatial positions\n",
        "    adata.obsm['spatial'] = np.array(df_pixels)\n",
        "    adata.obs['x_pos'] = adata.obsm['spatial'][:, 0]\n",
        "    adata.obs['y_pos'] = adata.obsm['spatial'][:, 1]\n",
        "    \n",
        "    # 3. Define Indices for Shuffling\n",
        "    label_type = ['L1', 'L2/3', 'L4', 'L5', 'L6', 'HPC/CC']\n",
        "    \n",
        "    # Find indices for each label type\n",
        "    index_all = [np.array([i for i in range(len(df_labels)) if df_labels[i] == label_type[0]])]\n",
        "    for k in range(1, len(label_type)):\n",
        "        temp_idx = np.array([i for i in range(len(df_labels)) if df_labels[i] == label_type[k]])\n",
        "        index_all.append(temp_idx)\n",
        "    \n",
        "    # Define shuffling groups (L4/L5 and L5/L6)\n",
        "    index_int1 = np.array(list(index_all[2]) + list(index_all[3]))\n",
        "    index_int2 = np.array(list(index_all[4]) + list(index_all[3]))\n",
        "    \n",
        "    # 4. Generate adata1 (Simulated RNA) and adata2 (Simulated Protein)\n",
        "    \n",
        "    # adata1: Adding Gaussian noise + Shuffling L4/L5\n",
        "    adata1 = adata.copy()\n",
        "    np.random.seed(RANDOM_SEED)\n",
        "    data_noise_1 = 1 + np.random.normal(0, 0.05, adata.shape)\n",
        "    adata1.X[index_int1, :] = np.multiply(adata.X, data_noise_1)[np.random.permutation(index_int1), :]\n",
        "    \n",
        "    # adata2: Adding Gaussian noise + Shuffling L5/L6\n",
        "    adata2 = adata.copy()\n",
        "    np.random.seed(RANDOM_SEED + 1)\n",
        "    data_noise_2 = 1 + np.random.normal(0, 0.05, adata.shape)\n",
        "    adata2.X[index_int2, :] = np.multiply(adata.X, data_noise_2)[np.random.permutation(index_int2), :]\n",
        "    \n",
        "    # 5. Format return variables to match original function signature\n",
        "    loc = np.array(adata1.obsm['spatial'])\n",
        "    LayerName = np.array(adata1.obs['LayerName'])\n",
        "    \n",
        "    print(f\"  - Cells: {adata1.shape[0]}\")\n",
        "    print(f\"  - RNA features: {adata1.shape[1]}\")\n",
        "    print(f\"  - Protein features: {adata2.shape[1]}\")\n",
        "    print(f\"  - Layers: {len(np.unique(LayerName))}\")\n",
        "    \n",
        "    return adata1, adata2, LayerName, loc, 'VisualCortex'\n",
        "\n",
        "def load_mob_data():\n",
        "    \"\"\"\n",
        "    Load D3: Mouse Olfactory Bulb dataset.\n",
        "    Splits single modality into two based on gene variance.\n",
        "    \"\"\"\n",
        "    print(\"\\nLoading D3: Mouse Olfactory Bulb...\")\n",
        "    \n",
        "    counts = pd.read_csv('./datasets/MOB_counts.csv', index_col=0)\n",
        "    metadata = pd.read_csv('./datasets/MOB_meta.csv', index_col=0)\n",
        "    \n",
        "    spatial_coords = metadata[['x_pos', 'y_pos']].values\n",
        "    layer_annotations = metadata['Layertype'].values\n",
        "    \n",
        "    # Create AnnData for preprocessing\n",
        "    adata_full = sc.AnnData(counts.values)\n",
        "    adata_full.obs['Layertype'] = layer_annotations\n",
        "    adata_full.obsm['spatial'] = spatial_coords\n",
        "    \n",
        "    # Preprocess\n",
        "    sc.pp.normalize_total(adata_full, target_sum=1e4)\n",
        "    sc.pp.log1p(adata_full)\n",
        "    \n",
        "    # Split by variance\n",
        "    gene_var = np.var(adata_full.X, axis=0)\n",
        "    if hasattr(gene_var, 'A1'):\n",
        "        gene_var = gene_var.A1\n",
        "    \n",
        "    var_threshold = np.median(gene_var)\n",
        "    high_var_genes = gene_var >= var_threshold\n",
        "    low_var_genes = ~high_var_genes\n",
        "    \n",
        "    # Create two modalities\n",
        "    if hasattr(adata_full.X, 'toarray'):\n",
        "        data1 = adata_full.X.toarray()[:, high_var_genes]\n",
        "        data2 = adata_full.X.toarray()[:, low_var_genes]\n",
        "    else:\n",
        "        data1 = adata_full.X[:, high_var_genes]\n",
        "        data2 = adata_full.X[:, low_var_genes]\n",
        "    \n",
        "    adata1 = sc.AnnData(data1, dtype='float64')\n",
        "    adata1.obs['Layertype'] = layer_annotations\n",
        "    adata1.obsm['spatial'] = spatial_coords\n",
        "    adata1.obs['x_pos'] = spatial_coords[:, 0]\n",
        "    adata1.obs['y_pos'] = spatial_coords[:, 1]\n",
        "    \n",
        "    adata2 = sc.AnnData(data2, dtype='float64')\n",
        "    adata2.obs['Layertype'] = layer_annotations\n",
        "    adata2.obsm['spatial'] = spatial_coords\n",
        "    adata2.obs['x_pos'] = spatial_coords[:, 0]\n",
        "    adata2.obs['y_pos'] = spatial_coords[:, 1]\n",
        "    \n",
        "    print(f\"  - Cells: {adata1.shape[0]}\")\n",
        "    print(f\"  - Modality 1 (high-var genes): {adata1.shape[1]}\")\n",
        "    print(f\"  - Modality 2 (low-var genes): {adata2.shape[1]}\")\n",
        "    print(f\"  - Layers: {len(np.unique(layer_annotations))}\")\n",
        "    \n",
        "    return adata1, adata2, layer_annotations, spatial_coords, 'MOB'\n",
        "\n",
        "\n",
        "print(\"‚úì Data loading functions defined\")\n",
        "\n",
        "def load_spatialglue_data():\n",
        "    \"\"\"\n",
        "    Load D4: SpatialGlue simulation dataset with ground truth factors.\n",
        "    Two modalities: ADT and RNA with spatial factor annotations.\n",
        "    \"\"\"\n",
        "    print(\"\\nLoading D4: SpatialGlue Simulation (ADT + RNA)...\")\n",
        "    \n",
        "    # Load the h5ad files\n",
        "    adt_ad = sc.read_h5ad(\"./datasets/spatialGlue_sim_adata_ADT.h5ad\")\n",
        "    rna_ad = sc.read_h5ad(\"./datasets/spatialGlue_sim_adata_RNA.h5ad\")\n",
        "    \n",
        "    # Extract data\n",
        "    adata1 = adt_ad.copy()  # ADT modality\n",
        "    adata2 = rna_ad.copy()  # RNA modality\n",
        "    \n",
        "    # Add ground truth labels from spatial factors\n",
        "    # Ground truth is encoded in spfac (spatial factors) in obsm\n",
        "    adata_genes = rna_ad.copy()  # Use RNA for ground truth computation\n",
        "    adata_genes.obs['ground_truth'] = (\n",
        "        1 * np.array(adata_genes.obsm['spfac'][:,0]) + \n",
        "        2 * np.array(adata_genes.obsm['spfac'][:,1]) + \n",
        "        3 * np.array(adata_genes.obsm['spfac'][:,2]) + \n",
        "        4 * np.array(adata_genes.obsm['spfac'][:,3])\n",
        "    )\n",
        "    \n",
        "    # Create annotation labels\n",
        "    adata_genes.obs['annotation'] = adata_genes.obs['ground_truth']\n",
        "    adata_genes.obs['annotation'] = adata_genes.obs['annotation'].replace({\n",
        "        1.0: 'factor1',\n",
        "        2.0: 'factor2',\n",
        "        3.0: 'factor3',\n",
        "        4.0: 'factor4',\n",
        "        0.0: 'backgr'  # background\n",
        "    })\n",
        "    \n",
        "    # Extract spatial coordinates and labels\n",
        "    spatial_coords = np.array(adata1.obsm['spatial'])\n",
        "    LayerName = adata_genes.obs['annotation'].values\n",
        "    \n",
        "    # Add spatial coordinates to both modalities\n",
        "    adata1.obs['x_pos'] = spatial_coords[:, 0]\n",
        "    adata1.obs['y_pos'] = spatial_coords[:, 1]\n",
        "    adata1.obs['LayerName'] = LayerName\n",
        "    \n",
        "    adata2.obs['x_pos'] = spatial_coords[:, 0]\n",
        "    adata2.obs['y_pos'] = spatial_coords[:, 1]\n",
        "    adata2.obs['LayerName'] = LayerName\n",
        "    \n",
        "    print(f\"  - Cells: {adata1.shape[0]}\")\n",
        "    print(f\"  - ADT features: {adata1.shape[1]}\")\n",
        "    print(f\"  - RNA features: {adata2.shape[1]}\")\n",
        "    print(f\"  - Factors: {len(np.unique(LayerName))}\")\n",
        "    print(f\"  - Factor distribution: {dict(zip(*np.unique(LayerName, return_counts=True)))}\")\n",
        "    \n",
        "    return adata1, adata2, LayerName, spatial_coords, 'SpatialGlue_Sim'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def load_spatial_multiomics_multi_simulated():\n",
        "    \"\"\"\n",
        "    Load D5: Spatial Multiomics Multi-Simulated dataset.\n",
        "    RNA + ADT modalities from a single h5ad file with domain annotations.\n",
        "    \n",
        "    Data structure:\n",
        "    - RNA: adata.X\n",
        "    - ADT: adata.obsm[\"ADT\"]\n",
        "    - Spatial: adata.obsm[\"spatial\"]\n",
        "    - Labels: adata.obs[\"domain\"] or adata.obs[\"ground_truth\"]\n",
        "    \"\"\"\n",
        "    print(\"\\nLoading D5: Spatial Multiomics Multi-Simulated...\")\n",
        "    \n",
        "    # Load the combined h5ad file\n",
        "    adt_rna_ad = sc.read_h5ad(\"./datasets/spatial_multiomics_multi_simulated.h5ad\")\n",
        "    \n",
        "    print(f\"  - Loaded AnnData: {adt_rna_ad.shape}\")\n",
        "    print(f\"  - Available keys in obsm: {list(adt_rna_ad.obsm.keys())}\")\n",
        "    print(f\"  - Available keys in obs: {list(adt_rna_ad.obs.keys())}\")\n",
        "    \n",
        "    # Extract RNA modality (main data matrix)\n",
        "    rna_data = adt_rna_ad.X\n",
        "    adata_rna = sc.AnnData(rna_data, dtype=\"float64\")\n",
        "    \n",
        "    # Extract ADT modality from obsm\n",
        "    adt_data = adt_rna_ad.obsm[\"ADT\"]\n",
        "    adata_adt = sc.AnnData(adt_data, dtype=\"float64\")\n",
        "    \n",
        "    # Extract spatial coordinates\n",
        "    spatial_coords = np.array(adt_rna_ad.obsm[\"spatial\"])\n",
        "    \n",
        "    # Extract ground truth labels (try 'domain' first, then 'ground_truth')\n",
        "    if \"domain\" in adt_rna_ad.obs.columns:\n",
        "        LayerName = adt_rna_ad.obs[\"domain\"].values\n",
        "    elif \"ground_truth\" in adt_rna_ad.obs.columns:\n",
        "        LayerName = adt_rna_ad.obs[\"ground_truth\"].values\n",
        "    else:\n",
        "        raise ValueError(\"No ground truth labels found. Expected 'domain' or 'ground_truth' in obs.\")\n",
        "    \n",
        "    # Add spatial coordinates and labels to both modalities\n",
        "    for adata in [adata_rna, adata_adt]:\n",
        "        adata.obsm['spatial'] = spatial_coords\n",
        "        adata.obs['x_pos'] = spatial_coords[:, 0]\n",
        "        adata.obs['y_pos'] = spatial_coords[:, 1]\n",
        "        adata.obs['LayerName'] = LayerName\n",
        "    \n",
        "    print(f\"  - Cells: {adata_rna.shape[0]}\")\n",
        "    print(f\"  - RNA features: {adata_rna.shape[1]}\")\n",
        "    print(f\"  - ADT features: {adata_adt.shape[1]}\")\n",
        "    print(f\"  - Spatial domains: {len(np.unique(LayerName))}\")\n",
        "    print(f\"  - Domain distribution: {dict(zip(*np.unique(LayerName, return_counts=True)))}\")\n",
        "    \n",
        "    # Return: RNA as adata1, ADT as adata2\n",
        "    return adata_rna, adata_adt, LayerName, spatial_coords, 'SpatialMultiomics_MultiSim'\n",
        "\n",
        "\n",
        "print(\"‚úì All dataset loading functions defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5. Main Analysis Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_dataset(dataset_id, gt_pe_config):\n",
        "    \"\"\"\n",
        "    Complete analysis pipeline for a single dataset.\n",
        "    \n",
        "    Parameters:\n",
        "    - dataset_id: 'D1', 'D2', 'D3', 'D4' or 'D5'\n",
        "    - gt_pe_config: Dictionary with GT+PE hyperparameters\n",
        "    \n",
        "    Returns:\n",
        "    - Dictionary with all results\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"ANALYZING DATASET {dataset_id}\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # Load data\n",
        "    if dataset_id == 'D1':\n",
        "        adata1, adata2, true_labels, spatial_coords, dataset_name = load_mousebrain_data()\n",
        "    elif dataset_id == 'D2':\n",
        "        adata1, adata2, true_labels, spatial_coords, dataset_name = load_visualcortex_data()\n",
        "    elif dataset_id == 'D3':\n",
        "        adata1, adata2, true_labels, spatial_coords, dataset_name = load_mob_data()\n",
        "    elif dataset_id == 'D4':\n",
        "        adata1, adata2, true_labels, spatial_coords, dataset_name = load_spatialglue_data()\n",
        "    elif dataset_id == 'D5':\n",
        "        adata1, adata2, true_labels, spatial_coords, dataset_name = load_spatial_multiomics_multi_simulated()\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown dataset: {dataset_id}\")\n",
        "    \n",
        "    results = {\n",
        "        'dataset_id': dataset_id,\n",
        "        'dataset_name': dataset_name,\n",
        "        'n_cells': adata1.shape[0],\n",
        "        'n_features_mod1': adata1.shape[1],\n",
        "        'n_features_mod2': adata2.shape[1],\n",
        "        'true_labels': true_labels,\n",
        "        'spatial_coords': spatial_coords\n",
        "    }\n",
        "    \n",
        "    # ============================================================\n",
        "    # TRAIN GCN (Original COSMOS)\n",
        "    # ============================================================\n",
        "    print(\"\\n\" + \"-\"*70)\n",
        "    print(\"TRAINING: GCN (Original COSMOS)\")\n",
        "    print(\"-\"*70)\n",
        "    \n",
        "    cosmos_gcn = Cosmos_GCN(adata1=adata1, adata2=adata2, save_inter_emb=True, save_fin_emb=True)\n",
        "    cosmos_gcn.preprocessing_data(n_neighbors=N_NEIGHBORS)\n",
        "    \n",
        "    embedding_gcn = cosmos_gcn.train(\n",
        "        spatial_regularization_strength=SPATIAL_REG_STRENGTH,\n",
        "        z_dim=GCN_Z_DIM,\n",
        "        lr=LEARNING_RATE,\n",
        "        wnn_epoch=WNN_EPOCH,\n",
        "        total_epoch=TOTAL_EPOCHS,\n",
        "        max_patience_bef=MAX_PATIENCE_BEF,\n",
        "        max_patience_aft=MAX_PATIENCE_AFT,\n",
        "        min_stop=MIN_STOP,\n",
        "        random_seed=RANDOM_SEED,\n",
        "        gpu=GPU_ID,\n",
        "        regularization_acceleration=REGULARIZATION_ACCELERATION,\n",
        "        edge_subset_sz=EDGE_SUBSET_SIZE\n",
        "    )\n",
        "    \n",
        "    weights_gcn = cosmos_gcn.weights\n",
        "    df_embedding_gcn = pd.DataFrame(embedding_gcn)\n",
        "    \n",
        "    # Clustering with optimal resolution (auto-tuned)\n",
        "    embedding_adata_gcn = sc.AnnData(df_embedding_gcn)\n",
        "    sc.pp.neighbors(embedding_adata_gcn, n_neighbors=NEIGHBORS_FOR_CLUSTERING, use_rep='X')\n",
        "    \n",
        "    gcn_clusters, gcn_res, gcn_ari = find_optimal_clustering(\n",
        "        embedding_adata_gcn, true_labels, CLUSTERING_RESOLUTIONS\n",
        "    )\n",
        "    \n",
        "    print(f\"\\n‚úì GCN training complete\")\n",
        "    print(f\"  - Optimal resolution: {gcn_res}\")\n",
        "    print(f\"  - Best ARI: {gcn_ari:.4f}\")\n",
        "    \n",
        "    # Compute all metrics\n",
        "    gcn_metrics = compute_all_metrics(true_labels, gcn_clusters, embedding_gcn)\n",
        "    gcn_metrics['Spatial_Coherence'] = compute_spatial_coherence(\n",
        "        spatial_coords, gcn_clusters, k=10\n",
        "    )\n",
        "    gcn_metrics['Optimal_Resolution'] = gcn_res\n",
        "    \n",
        "    print_metrics_table(gcn_metrics, \"GCN\")\n",
        "    \n",
        "    results['gcn'] = {\n",
        "        'embedding': embedding_gcn,\n",
        "        'weights': weights_gcn,\n",
        "        'clusters': gcn_clusters,\n",
        "        'metrics': gcn_metrics\n",
        "    }\n",
        "\n",
        "    cosmos_gcn.save_model()\n",
        "    \n",
        "    # ============================================================\n",
        "    # TRAIN GRAPH TRANSFORMER + PE\n",
        "    # ============================================================\n",
        "    print(\"\\n\" + \"-\"*70)\n",
        "    print(\"TRAINING: Graph Transformer + PE\")\n",
        "    print(\"-\"*70)\n",
        "    print(f\"Configuration:\")\n",
        "    for key, value in gt_pe_config.items():\n",
        "        print(f\"  - {key}: {value}\")\n",
        "    \n",
        "    cosmos_gt_pe = Cosmos_GT_PE(adata1=adata1, adata2=adata2, save_inter_emb=True, save_fin_emb=True)\n",
        "    cosmos_gt_pe.preprocessing_data(n_neighbors=N_NEIGHBORS)\n",
        "    \n",
        "    embedding_gt_pe = cosmos_gt_pe.train(\n",
        "        spatial_regularization_strength=SPATIAL_REG_STRENGTH,\n",
        "        z_dim=gt_pe_config['z_dim'],\n",
        "        lr=LEARNING_RATE,\n",
        "        wnn_epoch=WNN_EPOCH,\n",
        "        total_epoch=TOTAL_EPOCHS,\n",
        "        max_patience_bef=MAX_PATIENCE_BEF,\n",
        "        max_patience_aft=MAX_PATIENCE_AFT,\n",
        "        min_stop=MIN_STOP,\n",
        "        random_seed=RANDOM_SEED,\n",
        "        gpu=GPU_ID,\n",
        "        regularization_acceleration=REGULARIZATION_ACCELERATION,\n",
        "        edge_subset_sz=EDGE_SUBSET_SIZE,\n",
        "        # Graph Transformer parameters\n",
        "        num_heads=gt_pe_config['num_heads'],\n",
        "        dropout=gt_pe_config['dropout'],\n",
        "        pe_dim=gt_pe_config['pe_dim'],\n",
        "        use_pe=gt_pe_config['use_pe']\n",
        "    )\n",
        "    \n",
        "    weights_gt_pe = cosmos_gt_pe.weights\n",
        "    df_embedding_gt_pe = pd.DataFrame(embedding_gt_pe)\n",
        "    \n",
        "    # Clustering with optimal resolution (auto-tuned)\n",
        "    embedding_adata_gt_pe = sc.AnnData(df_embedding_gt_pe)\n",
        "    sc.pp.neighbors(embedding_adata_gt_pe, n_neighbors=NEIGHBORS_FOR_CLUSTERING, use_rep='X')\n",
        "    \n",
        "    gt_pe_clusters, gt_pe_res, gt_pe_ari = find_optimal_clustering(\n",
        "        embedding_adata_gt_pe, true_labels, CLUSTERING_RESOLUTIONS\n",
        "    )\n",
        "    \n",
        "    print(f\"\\n‚úì GT+PE training complete\")\n",
        "    print(f\"  - Optimal resolution: {gt_pe_res}\")\n",
        "    print(f\"  - Best ARI: {gt_pe_ari:.4f}\")\n",
        "    \n",
        "    # Compute all metrics\n",
        "    gt_pe_metrics = compute_all_metrics(true_labels, gt_pe_clusters, embedding_gt_pe)\n",
        "    gt_pe_metrics['Spatial_Coherence'] = compute_spatial_coherence(\n",
        "        spatial_coords, gt_pe_clusters, k=10\n",
        "    )\n",
        "    gt_pe_metrics['Optimal_Resolution'] = gt_pe_res\n",
        "    \n",
        "    print_metrics_table(gt_pe_metrics, \"Graph Transformer + PE\")\n",
        "    \n",
        "    results['gt_pe'] = {\n",
        "        'embedding': embedding_gt_pe,\n",
        "        'weights': weights_gt_pe,\n",
        "        'clusters': gt_pe_clusters,\n",
        "        'metrics': gt_pe_metrics,\n",
        "        'config': gt_pe_config\n",
        "    }\n",
        "\n",
        "    cosmos_gt_pe.save_model()\n",
        "    \n",
        "    # ============================================================\n",
        "    # COMPARISON AND VISUALIZATION\n",
        "    # ============================================================\n",
        "    print(\"\\n\" + \"-\"*70)\n",
        "    print(\"GENERATING VISUALIZATIONS\")\n",
        "    print(\"-\"*70)\n",
        "    \n",
        "    # Spatial comparison\n",
        "    fig = create_comparison_plot(\n",
        "        adata1, spatial_coords, true_labels,\n",
        "        gcn_clusters, gt_pe_clusters,\n",
        "        gcn_ari, gt_pe_ari, dataset_name\n",
        "    )\n",
        "    fig.savefig(f'./outputs/{dataset_id}_{dataset_name}_spatial_comparison.png', \n",
        "                dpi=FIGURE_DPI, bbox_inches='tight')\n",
        "    plt.close(fig)\n",
        "    print(f\"  ‚úì Saved: {dataset_id}_{dataset_name}_spatial_comparison.png\")\n",
        "    \n",
        "    # UMAP comparison\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "    \n",
        "    # UMAP for GCN\n",
        "    umap_gcn = UMAP(n_components=2, init='random', random_state=RANDOM_SEED,\n",
        "                    min_dist=UMAP_MIN_DIST, n_neighbors=UMAP_N_NEIGHBORS)\n",
        "    umap_pos_gcn = umap_gcn.fit_transform(df_embedding_gcn)\n",
        "    \n",
        "    ax = axes[0]\n",
        "    plot_colors = ['#D1D1D1', '#e6194b', '#3cb44b', '#ffe119', '#4363d8', \n",
        "                   '#f58231', '#911eb4', '#46f0f0', '#f032e6', '#bcf60c']\n",
        "    for i, cluster in enumerate(np.unique(gcn_clusters)):\n",
        "        mask = gcn_clusters == cluster\n",
        "        ax.scatter(umap_pos_gcn[mask, 0], umap_pos_gcn[mask, 1],\n",
        "                   c=plot_colors[i % len(plot_colors)], label=str(cluster),\n",
        "                   s=POINT_SIZE_UMAP, alpha=0.6)\n",
        "    ax.set_title(f'UMAP: GCN (ARI = {gcn_ari:.3f})', fontweight='bold')\n",
        "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', frameon=False, fontsize=8)\n",
        "    ax.axis('off')\n",
        "    \n",
        "    # UMAP for GT+PE\n",
        "    umap_gt_pe = UMAP(n_components=2, init='random', random_state=RANDOM_SEED,\n",
        "                      min_dist=UMAP_MIN_DIST, n_neighbors=UMAP_N_NEIGHBORS)\n",
        "    umap_pos_gt_pe = umap_gt_pe.fit_transform(df_embedding_gt_pe)\n",
        "    \n",
        "    ax = axes[1]\n",
        "    for i, cluster in enumerate(np.unique(gt_pe_clusters)):\n",
        "        mask = gt_pe_clusters == cluster\n",
        "        ax.scatter(umap_pos_gt_pe[mask, 0], umap_pos_gt_pe[mask, 1],\n",
        "                   c=plot_colors[i % len(plot_colors)], label=str(cluster),\n",
        "                   s=POINT_SIZE_UMAP, alpha=0.6)\n",
        "    ax.set_title(f'UMAP: GT+PE (ARI = {gt_pe_ari:.3f})', fontweight='bold')\n",
        "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', frameon=False, fontsize=8)\n",
        "    ax.axis('off')\n",
        "    \n",
        "    plt.suptitle(f'{dataset_name} - UMAP Comparison', fontsize=12, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    fig.savefig(f'./outputs/{dataset_id}_{dataset_name}_UMAP_comparison.png',\n",
        "                dpi=FIGURE_DPI, bbox_inches='tight')\n",
        "    plt.close(fig)\n",
        "    print(f\"  ‚úì Saved: {dataset_id}_{dataset_name}_UMAP_comparison.png\")\n",
        "    \n",
        "    # ============================================================\n",
        "    # FINAL SUMMARY\n",
        "    # ============================================================\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"FINAL RESULTS: {dataset_name}\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    print(f\"\\n{'Metric':<25} {'GCN':>12} {'GT+PE':>12} {'Improvement':>12}\")\n",
        "    print(\"-\"*70)\n",
        "    \n",
        "    for metric in ['ARI', 'NMI', 'Silhouette', 'Spatial_Coherence']:\n",
        "        gcn_val = gcn_metrics[metric]\n",
        "        gt_pe_val = gt_pe_metrics[metric]\n",
        "        \n",
        "        if isinstance(gcn_val, float) and not np.isnan(gcn_val):\n",
        "            improvement = ((gt_pe_val - gcn_val) / gcn_val * 100) if gcn_val != 0 else 0\n",
        "            print(f\"{metric:<25} {gcn_val:>12.4f} {gt_pe_val:>12.4f} {improvement:>11.1f}%\")\n",
        "    \n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    if gt_pe_ari > gcn_ari:\n",
        "        improvement = ((gt_pe_ari - gcn_ari) / gcn_ari * 100)\n",
        "        print(f\"\\n‚úì WINNER: Graph Transformer + PE (+{improvement:.1f}% ARI improvement)\")\n",
        "        print(f\"  ‚Üí This dataset benefits from global attention and topology awareness\")\n",
        "    elif gcn_ari > gt_pe_ari:\n",
        "        improvement = ((gcn_ari - gt_pe_ari) / gt_pe_ari * 100)\n",
        "        print(f\"\\n‚úì WINNER: GCN (+{improvement:.1f}% ARI improvement)\")\n",
        "        print(f\"  ‚Üí This dataset has strong local/layered structure\")\n",
        "    else:\n",
        "        print(f\"\\n‚âà TIE: Both methods perform similarly\")\n",
        "    \n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "\n",
        "print(\"‚úì Main analysis function defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 6. Run Analysis on Selected Dataset(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Store all results\n",
        "all_results = {}\n",
        "\n",
        "if DATASET_TO_RUN == 'ALL':\n",
        "    datasets_to_analyze = ['D1', 'D2', 'D3', 'D4', 'D5']\n",
        "else:\n",
        "    datasets_to_analyze = [DATASET_TO_RUN]\n",
        "\n",
        "for dataset_id in datasets_to_analyze:\n",
        "    # Select appropriate configuration\n",
        "    if dataset_id == 'D1':\n",
        "        gt_pe_config = GT_PE_D1_CONFIG\n",
        "    elif dataset_id == 'D2':\n",
        "        gt_pe_config = GT_PE_D2_CONFIG\n",
        "    elif dataset_id == 'D3':\n",
        "        gt_pe_config = GT_PE_D3_CONFIG\n",
        "    elif dataset_id == 'D4':\n",
        "        gt_pe_config = GT_PE_D4_CONFIG\n",
        "    elif dataset_id == 'D5':\n",
        "        gt_pe_config = GT_PE_D5_CONFIG\n",
        "    \n",
        "    # Run analysis\n",
        "    try:\n",
        "        results = analyze_dataset(dataset_id, gt_pe_config)\n",
        "        all_results[dataset_id] = results\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Error analyzing {dataset_id}: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ALL ANALYSES COMPLETE\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 7. Cross-Dataset Comparison Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if len(all_results) > 1:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"CROSS-DATASET COMPARISON SUMMARY\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Create summary table\n",
        "    summary_data = []\n",
        "    \n",
        "    for dataset_id, results in all_results.items():\n",
        "        gcn_ari = results['gcn']['metrics']['ARI']\n",
        "        gt_pe_ari = results['gt_pe']['metrics']['ARI']\n",
        "        improvement = ((gt_pe_ari - gcn_ari) / gcn_ari * 100) if gcn_ari != 0 else 0\n",
        "        winner = 'GT+PE' if gt_pe_ari > gcn_ari else 'GCN' if gcn_ari > gt_pe_ari else 'Tie'\n",
        "        \n",
        "        summary_data.append({\n",
        "            'Dataset': results['dataset_name'],\n",
        "            'Cells': results['n_cells'],\n",
        "            'GCN_ARI': gcn_ari,\n",
        "            'GT_PE_ARI': gt_pe_ari,\n",
        "            'Improvement_%': improvement,\n",
        "            'Winner': winner\n",
        "        })\n",
        "    \n",
        "    df_summary = pd.DataFrame(summary_data)\n",
        "    print(\"\\n\" + df_summary.to_string(index=False))\n",
        "    \n",
        "    # Save summary\n",
        "    df_summary.to_csv('./outputs/cross_dataset_summary.csv', index=False)\n",
        "    print(\"\\n‚úì Summary saved to: cross_dataset_summary.csv\")\n",
        "    \n",
        "    # Overall statistics\n",
        "    gt_pe_wins = (df_summary['Winner'] == 'GT+PE').sum()\n",
        "    gcn_wins = (df_summary['Winner'] == 'GCN').sum()\n",
        "    \n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "    print(\"OVERALL STATISTICS:\")\n",
        "    print(f\"  - GT+PE wins: {gt_pe_wins}/{len(all_results)} datasets\")\n",
        "    print(f\"  - GCN wins: {gcn_wins}/{len(all_results)} datasets\")\n",
        "    print(f\"  - Average improvement (GT+PE over GCN): {df_summary['Improvement_%'].mean():.1f}%\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Visualization: Bar plot\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    x = np.arange(len(df_summary))\n",
        "    width = 0.35\n",
        "    \n",
        "    bars1 = ax.bar(x - width/2, df_summary['GCN_ARI'], width, label='GCN', alpha=0.8)\n",
        "    bars2 = ax.bar(x + width/2, df_summary['GT_PE_ARI'], width, label='GT+PE', alpha=0.8)\n",
        "    \n",
        "    ax.set_xlabel('Dataset', fontweight='bold')\n",
        "    ax.set_ylabel('ARI', fontweight='bold')\n",
        "    ax.set_title('Cross-Dataset Performance Comparison', fontweight='bold', fontsize=14)\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(df_summary['Dataset'], rotation=45, ha='right')\n",
        "    ax.legend()\n",
        "    ax.grid(axis='y', alpha=0.3)\n",
        "    ax.set_ylim([0, 1])\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"./outputs\" / 'cross_dataset_comparison.png', dpi=FIGURE_DPI, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\n‚úì Comparison plot saved to: cross_dataset_comparison.png\")\n",
        "\n",
        "else:\n",
        "    print(\"\\n‚úì Single dataset analysis complete\")\n",
        "    print(f\"  Results for {DATASET_TO_RUN} saved\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 8. Export Results to CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export detailed metrics for each dataset\n",
        "for dataset_id, results in all_results.items():\n",
        "    # Create detailed results dataframe\n",
        "    detailed_results = []\n",
        "    \n",
        "    for method in ['gcn', 'gt_pe']:\n",
        "        method_name = 'GCN' if method == 'gcn' else 'Graph Transformer + PE'\n",
        "        metrics = results[method]['metrics']\n",
        "        \n",
        "        row = {\n",
        "            'Dataset': results['dataset_name'],\n",
        "            'Method': method_name,\n",
        "            'Cells': results['n_cells'],\n",
        "            'Features_Mod1': results['n_features_mod1'],\n",
        "            'Features_Mod2': results['n_features_mod2']\n",
        "        }\n",
        "        \n",
        "        # Add all metrics\n",
        "        row.update(metrics)\n",
        "        \n",
        "        # Add hyperparameters for GT+PE\n",
        "        if method == 'gt_pe':\n",
        "            config = results[method]['config']\n",
        "            row.update({\n",
        "                f'Hyperparam_{k}': v for k, v in config.items()\n",
        "            })\n",
        "        \n",
        "        detailed_results.append(row)\n",
        "    \n",
        "    df_detailed = pd.DataFrame(detailed_results)\n",
        "    filename = f'./outputs/{dataset_id}_{results[\"dataset_name\"]}_detailed_metrics.csv'\n",
        "    df_detailed.to_csv(filename, index=False)\n",
        "    print(f\"‚úì Saved: {filename}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ALL RESULTS EXPORTED\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nGenerated files:\")\n",
        "print(\"  - Spatial comparison PNGs (one per dataset)\")\n",
        "print(\"  - UMAP comparison PNGs (one per dataset)\")\n",
        "print(\"  - Detailed metrics CSVs (one per dataset)\")\n",
        "if len(all_results) > 1:\n",
        "    print(\"  - cross_dataset_summary.csv\")\n",
        "    print(\"  - cross_dataset_comparison.png\")\n",
        "print(\"\\n‚úì Analysis pipeline complete!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "py3.10",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
